Combined Python Files - Generated on 2025-01-18 00:22:54

Directory: I:\threat-detection-fastapi\app

================================================================================


File: _init_.py
===============


================================================================================


File: main.py
=============
# app/main.py
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from pathlib import Path
import cv2
import numpy as np
from PIL import Image
import io
import tempfile
from typing import List, Optional
import logging
import traceback

from app.models import DetectionResponse, DetectionResult
from app.services import ModelService
from app.settings import *

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Threat Detection API",
    description="API for detecting threats in images and videos using YOLOv8",
    version="1.0.0"
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize model service
model_service = None

@app.on_event("startup")
async def startup_event():
    """Initialize the model on startup"""
    global model_service
    try:
        logger.info("Initializing model service...")
        model_service = ModelService()
        model_path = Path(DETECTION_MODEL)
        if not model_path.exists():
            raise FileNotFoundError(f"Model file not found at {model_path}")
        model_service.initialize_model(str(model_path))
        logger.info("Model initialized successfully")
    except Exception as e:
        logger.error(f"Failed to initialize model: {str(e)}")
        logger.error(traceback.format_exc())
        raise

@app.get("/")
async def root():
    """Root endpoint to check API status"""
    return {"status": "online", "message": "Threat Detection API is running"}

def process_image(contents: bytes) -> np.ndarray:
    """Process uploaded image bytes into numpy array"""
    try:
        # First try using PIL
        image = Image.open(io.BytesIO(contents))
        image = image.convert('RGB')
        return np.array(image)
    except Exception as pil_error:
        logger.warning(f"PIL failed to open image: {str(pil_error)}")
        try:
            # Fallback to OpenCV
            nparr = np.frombuffer(contents, np.uint8)
            image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
            if image is None:
                raise ValueError("OpenCV failed to decode image")
            # Convert BGR to RGB
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            return image
        except Exception as cv_error:
            logger.error(f"OpenCV failed to open image: {str(cv_error)}")
            raise ValueError("Failed to process image with both PIL and OpenCV")

@app.post("/detect/image", response_model=DetectionResponse)
async def detect_image(
    file: UploadFile = File(...),
    confidence: float = 0.5
):
    """Detect threats in an uploaded image"""
    global model_service
    
    try:
        logger.info(f"Processing image detection request: {file.filename}")
        
        # Validate model service
        if model_service is None:
            raise RuntimeError("Model service not initialized")
        
        # Validate file type
        if file.content_type not in SUPPORTED_IMAGE_TYPES:
            raise HTTPException(
                status_code=400,
                detail=f"Invalid file type. Supported types: {SUPPORTED_IMAGE_TYPES}"
            )
        
        # Read and process image
        try:
            contents = await file.read()
            logger.info(f"Image size: {len(contents)} bytes")
            
            # Process image using our helper function
            image = process_image(contents)
            
            if image is None or image.size == 0:
                raise ValueError("Failed to load image")
                
            logger.info(f"Successfully loaded image with shape: {image.shape}")
            
        except Exception as e:
            logger.error(f"Error reading image: {str(e)}")
            logger.error(traceback.format_exc())
            raise HTTPException(status_code=400, detail=f"Invalid image file: {str(e)}")
        
        # Validate confidence
        if not 0 <= confidence <= 1:
            raise HTTPException(
                status_code=400,
                detail="Confidence must be between 0 and 1"
            )
        
        # Perform detection
        logger.info("Performing detection...")
        results = model_service.detect_image(image, confidence)
        logger.info(f"Detection completed. Found {len(results.detections)} threats")
        
        return results
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing image: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(
            status_code=500,
            detail=f"Internal server error: {str(e)}"
        )

@app.get("/model/info")
async def get_model_info():
    """Get information about the loaded model"""
    global model_service
    try:
        return {
            "model_path": str(DETECTION_MODEL),
            "status": "loaded" if model_service and model_service.model else "not_loaded",
            "initialized": model_service is not None
        }
    except Exception as e:
        logger.error(f"Error getting model info: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)

================================================================================


File: models.py
===============
from pathlib import Path
import sys
from pydantic import BaseModel
from typing import List, Optional

class DetectionResult(BaseModel):
    """Single detection result from the model"""
    class_id: int
    class_name: str
    confidence: float
    bbox: List[float]  # [x1, y1, x2, y2]

class DetectionResponse(BaseModel):
    """Response model for detection endpoints"""
    detections: List[DetectionResult]
    processing_time: float
# Get the absolute path of the current file
file_path = Path(__file__).resolve()
root_path = file_path.parent

# Add the root path to the sys.path list if it is not already there
if root_path not in sys.path:
    sys.path.append(str(root_path))

# ML Model config
DETECTION_MODEL = root_path / 'data' / 'models' / 'BestFinal.pt'

# API Settings
MAX_UPLOAD_SIZE = 100 * 1024 * 1024  # 100MB
SUPPORTED_IMAGE_TYPES = ["image/jpeg", "image/png", "image/bmp", "image/webp"]
SUPPORTED_VIDEO_TYPES = ["video/mp4", "video/x-msvideo"]

# Logging config
LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
LOG_LEVEL = 'INFO'

================================================================================


File: services.py
=================
# app/services.py
from pathlib import Path
import cv2
import numpy as np
from PIL import Image
import time
from ultralytics import YOLO
from typing import List, Optional, Union
import logging

from app.models import DetectionResponse, DetectionResult

logger = logging.getLogger(__name__)

class ModelService:
    def __init__(self):
        self.model = None

    def initialize_model(self, model_path: str):
        """Initialize the YOLO model"""
        try:
            logger.info(f"Loading model from {model_path}")
            if not Path(model_path).exists():
                raise FileNotFoundError(f"Model file not found at {model_path}")
            
            self.model = YOLO(model_path)
            logger.info("Model loaded successfully")
        except Exception as e:
            logger.error(f"Failed to load model: {str(e)}")
            raise

    def _process_results(self, results) -> DetectionResponse:
        """Process YOLO results into structured response"""
        try:
            detections = []
            
            for result in results:
                boxes = result.boxes
                for box in boxes:
                    detection = DetectionResult(
                        class_id=int(box.cls),
                        class_name=result.names[int(box.cls)],
                        confidence=float(box.conf),
                        bbox=box.xyxy[0].tolist()
                    )
                    detections.append(detection)

            return DetectionResponse(
                detections=detections,
                processing_time=results[0].speed.get('inference', 0)
            )
        except Exception as e:
            logger.error(f"Error processing results: {str(e)}")
            raise

    def detect_image(self, image: Union[np.ndarray, Image.Image], conf: float) -> DetectionResponse:
        """Perform detection on a single image"""
        try:
            if self.model is None:
                raise RuntimeError("Model not initialized")

            # Convert PIL Image to numpy array if needed
            if isinstance(image, Image.Image):
                image = np.array(image)

            # Ensure image is in correct format
            if image.dtype != np.uint8:
                image = image.astype(np.uint8)

            logger.info(f"Running inference on image with shape: {image.shape}")
            results = self.model.predict(image, conf=conf)
            logger.info("Inference completed")
            
            return self._process_results(results)
            
        except Exception as e:
            logger.error(f"Error in detect_image: {str(e)}")
            raise

================================================================================


File: settings.py
=================
from pathlib import Path
import sys

# Get the absolute path of the current file
file_path = Path(__file__).resolve()
root_path = file_path.parent

# Add the root path to the sys.path list if it is not already there
if root_path not in sys.path:
    sys.path.append(str(root_path))

# ML Model config
DETECTION_MODEL = root_path / 'data' / 'models' / 'BestFinal.pt'

# API Settings
MAX_UPLOAD_SIZE = 100 * 1024 * 1024  # 100MB
SUPPORTED_IMAGE_TYPES = ["image/jpeg", "image/png", "image/bmp", "image/webp"]
SUPPORTED_VIDEO_TYPES = ["video/mp4", "video/x-msvideo"]

# Logging config
LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
LOG_LEVEL = 'INFO'

================================================================================

